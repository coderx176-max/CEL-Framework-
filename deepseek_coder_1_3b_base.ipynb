{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2739456193df4faab7a1c7c7c84a2375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89f805c3d76f460d951a9b5babe0d767",
              "IPY_MODEL_9582b930011e4e129846468bec2c8ab4",
              "IPY_MODEL_33fff551cd0949c1b2d1207ea519c30b"
            ],
            "layout": "IPY_MODEL_29f543dc20594141b00b5216e941515a"
          }
        },
        "89f805c3d76f460d951a9b5babe0d767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6d1f35da4a743a28a9dc603a35e780f",
            "placeholder": "​",
            "style": "IPY_MODEL_5d9df8e6394240949cc9849af93ce53c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "9582b930011e4e129846468bec2c8ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1335178132914f69a48185468a305803",
            "max": 793,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7089f7d501e447f1829d4ba497af4c8d",
            "value": 793
          }
        },
        "33fff551cd0949c1b2d1207ea519c30b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d28268a0d9724d3cacb959eb05ecb60a",
            "placeholder": "​",
            "style": "IPY_MODEL_c26c442a5730401e801f810281ad7cc6",
            "value": " 793/793 [00:00&lt;00:00, 158kB/s]"
          }
        },
        "29f543dc20594141b00b5216e941515a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6d1f35da4a743a28a9dc603a35e780f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d9df8e6394240949cc9849af93ce53c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1335178132914f69a48185468a305803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7089f7d501e447f1829d4ba497af4c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d28268a0d9724d3cacb959eb05ecb60a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c26c442a5730401e801f810281ad7cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89ecef33c6224d0a8d8db7fca9cc0b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_034d20eac4fd45f2a6d283b7a745d0e4",
              "IPY_MODEL_b618be90895a4370987c039ac2234fdc",
              "IPY_MODEL_698359e794b840aaa9292a9a3d05cfda"
            ],
            "layout": "IPY_MODEL_ef98761c28c048dabf8e5cc83a993b92"
          }
        },
        "034d20eac4fd45f2a6d283b7a745d0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b5875f09a644597aa1e1a0753656cb5",
            "placeholder": "​",
            "style": "IPY_MODEL_24386ac035814db98574667ebf1e5ea5",
            "value": "tokenizer.json: "
          }
        },
        "b618be90895a4370987c039ac2234fdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_806748f77ade42d29e460ffd10a7d2bd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1fcb0ea830754a5cbd8cbe435951963e",
            "value": 1
          }
        },
        "698359e794b840aaa9292a9a3d05cfda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d315a0b18fea4942ad8b290aa3235b40",
            "placeholder": "​",
            "style": "IPY_MODEL_10086424f1b2470eb41b86732619b4ba",
            "value": " 1.37M/? [00:00&lt;00:00, 37.7MB/s]"
          }
        },
        "ef98761c28c048dabf8e5cc83a993b92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5875f09a644597aa1e1a0753656cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24386ac035814db98574667ebf1e5ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "806748f77ade42d29e460ffd10a7d2bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1fcb0ea830754a5cbd8cbe435951963e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d315a0b18fea4942ad8b290aa3235b40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10086424f1b2470eb41b86732619b4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ed77893bf694b20b313b11d01b7a79d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c007eed97c54aeca1a898ab07bd6f50",
              "IPY_MODEL_4d6b8d4ed14b43f1a4d47d4a41eee86f",
              "IPY_MODEL_30f7a0be25cb432ab55023c936600a4c"
            ],
            "layout": "IPY_MODEL_b20dbf70b37e4ea2a895133afdf13da9"
          }
        },
        "0c007eed97c54aeca1a898ab07bd6f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cca8200283d4958b2d43e8710e967ae",
            "placeholder": "​",
            "style": "IPY_MODEL_2b55ee615af14053ba3a58ed9bbc5f2b",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "4d6b8d4ed14b43f1a4d47d4a41eee86f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41322396a18f49d697289ff63a190cc6",
            "max": 482,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_577a9922421c408e87301a4948be8a33",
            "value": 482
          }
        },
        "30f7a0be25cb432ab55023c936600a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54716dcd2f2e4fe0aabd88a4ae49d2a5",
            "placeholder": "​",
            "style": "IPY_MODEL_11974ebd72ee480ab8f1510e4d4ee433",
            "value": " 482/482 [00:00&lt;00:00, 97.2kB/s]"
          }
        },
        "b20dbf70b37e4ea2a895133afdf13da9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cca8200283d4958b2d43e8710e967ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b55ee615af14053ba3a58ed9bbc5f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41322396a18f49d697289ff63a190cc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "577a9922421c408e87301a4948be8a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54716dcd2f2e4fe0aabd88a4ae49d2a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11974ebd72ee480ab8f1510e4d4ee433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e242d6b20d664b46a8a3dbdcf5cc34db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07f79c45bddd43f2bad5631fc0e30d37",
              "IPY_MODEL_f84f574e89854a8682236d8138797916",
              "IPY_MODEL_471d480ce6394c74bec51d751f484d0b"
            ],
            "layout": "IPY_MODEL_e450defc6e934f88acd756654660a295"
          }
        },
        "07f79c45bddd43f2bad5631fc0e30d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d40d9c3c70d8415895356826915ccb4c",
            "placeholder": "​",
            "style": "IPY_MODEL_ac24ed68a66c4974b9a73e786ab10d1e",
            "value": "config.json: 100%"
          }
        },
        "f84f574e89854a8682236d8138797916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b297540c57f44ae9efd43294c2b80d2",
            "max": 631,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e652e4350d67484782288d7959214ba0",
            "value": 631
          }
        },
        "471d480ce6394c74bec51d751f484d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63a1c85dc7454d31aac028cf209e8a1d",
            "placeholder": "​",
            "style": "IPY_MODEL_a5cda010ff84435284f80212955804a3",
            "value": " 631/631 [00:00&lt;00:00, 124kB/s]"
          }
        },
        "e450defc6e934f88acd756654660a295": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d40d9c3c70d8415895356826915ccb4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac24ed68a66c4974b9a73e786ab10d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b297540c57f44ae9efd43294c2b80d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e652e4350d67484782288d7959214ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63a1c85dc7454d31aac028cf209e8a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5cda010ff84435284f80212955804a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "880e11b958044e88ab47caa1d3fe1966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_437a021bca5d4ef0b8e1a3c487df4e91",
              "IPY_MODEL_8b5994c1f45549d3ae414179cbb3a6e2",
              "IPY_MODEL_e2001f411dc54cd1802efb37dd2b8400"
            ],
            "layout": "IPY_MODEL_3d3c99ad470f4f5886818432e480826f"
          }
        },
        "437a021bca5d4ef0b8e1a3c487df4e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f80429226be4bdbb74377ded4656902",
            "placeholder": "​",
            "style": "IPY_MODEL_3c016e00195749008707b9b2f391a883",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8b5994c1f45549d3ae414179cbb3a6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70e899dce4964ccea23dab2c9290fc87",
            "max": 2693014838,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6afdce93ecd94be7bf8ee134364b7265",
            "value": 2693014838
          }
        },
        "e2001f411dc54cd1802efb37dd2b8400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dacbac2601c4af38d1e033751edc037",
            "placeholder": "​",
            "style": "IPY_MODEL_2a079cb26702492489cef98de1a04d86",
            "value": " 2.69G/2.69G [00:18&lt;00:00, 133MB/s]"
          }
        },
        "3d3c99ad470f4f5886818432e480826f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f80429226be4bdbb74377ded4656902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c016e00195749008707b9b2f391a883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70e899dce4964ccea23dab2c9290fc87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6afdce93ecd94be7bf8ee134364b7265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2dacbac2601c4af38d1e033751edc037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a079cb26702492489cef98de1a04d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56d386e21dba41a4a07971e8d21ead6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67dbb1f211924d52b46a17a5b957f1e4",
              "IPY_MODEL_1dc9e49cf8b0441593915925ca1595fe",
              "IPY_MODEL_10c066643e074dfbbfe922e409abe41b"
            ],
            "layout": "IPY_MODEL_705da69fac0f4b969e6a17d20feb64b0"
          }
        },
        "67dbb1f211924d52b46a17a5b957f1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3a0e11eba6c46bf91bf6353a4cc4ed8",
            "placeholder": "​",
            "style": "IPY_MODEL_75016ac567614168963517082343208c",
            "value": "model.safetensors: 100%"
          }
        },
        "1dc9e49cf8b0441593915925ca1595fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e8b71f26b0d42488643c73bea2b6271",
            "max": 2692969128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7e7051ee6424aa8ae7a13712525ba77",
            "value": 2692969128
          }
        },
        "10c066643e074dfbbfe922e409abe41b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e6aeaff1ec44538f31b1f84d993eb4",
            "placeholder": "​",
            "style": "IPY_MODEL_59501bc60f8f49729daf75b58dc9462f",
            "value": " 2.69G/2.69G [00:23&lt;00:00, 115MB/s]"
          }
        },
        "705da69fac0f4b969e6a17d20feb64b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3a0e11eba6c46bf91bf6353a4cc4ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75016ac567614168963517082343208c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e8b71f26b0d42488643c73bea2b6271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7e7051ee6424aa8ae7a13712525ba77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23e6aeaff1ec44538f31b1f84d993eb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59501bc60f8f49729daf75b58dc9462f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e436fc2d139463dafea6f9f684fafb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_441bf22270ab41c687670629fd7c14a4",
              "IPY_MODEL_41e2e7576d0d4cfa8e13f91daeae905d",
              "IPY_MODEL_2c019f4c0a4845cd993a99ea517065cc"
            ],
            "layout": "IPY_MODEL_3a4e54f2897746bf8aead99ba034ea82"
          }
        },
        "441bf22270ab41c687670629fd7c14a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fb802ffcba247c281a5b79145c8a949",
            "placeholder": "​",
            "style": "IPY_MODEL_19e519a45fdd47b6b9c5df3da10d3f42",
            "value": "generation_config.json: 100%"
          }
        },
        "41e2e7576d0d4cfa8e13f91daeae905d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94a1f0edc9b4493fa731060d45ad83df",
            "max": 119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bab14d96b224a4897e827466a0ea5ec",
            "value": 119
          }
        },
        "2c019f4c0a4845cd993a99ea517065cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_277406fc10ae4327b2b24fcf934411ef",
            "placeholder": "​",
            "style": "IPY_MODEL_1c946ad33d98441ebe071f868c44114f",
            "value": " 119/119 [00:00&lt;00:00, 1.56kB/s]"
          }
        },
        "3a4e54f2897746bf8aead99ba034ea82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fb802ffcba247c281a5b79145c8a949": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e519a45fdd47b6b9c5df3da10d3f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94a1f0edc9b4493fa731060d45ad83df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bab14d96b224a4897e827466a0ea5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "277406fc10ae4327b2b24fcf934411ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c946ad33d98441ebe071f868c44114f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coderx176-max/CEL-Framework-/blob/main/deepseek_coder_1_3b_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "jzpNIh3DfGlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base\n",
        "\n",
        "⚠️ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) 🙏"
      ],
      "metadata": {
        "id": "Y25xXR1ofGlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/deepseek-coder-1.3b-base\")"
      ],
      "metadata": {
        "id": "3xZj3X4TfGlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")"
      ],
      "metadata": {
        "id": "Dp3t4lrkfGlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff3a9078"
      },
      "source": [
        "# Task\n",
        "Explain the fine-tuning process for creating an AI capable of generating full, functional APK files from a natural language description, then begin the fine-tuning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d422070c"
      },
      "source": [
        "## Select and prepare a base model\n",
        "\n",
        "### Subtask:\n",
        "Choose a suitable base model (e.g., a large language model pre-trained on code) and prepare it for fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56b1dee3"
      },
      "source": [
        "**Reasoning**:\n",
        "The base model and tokenizer have already been loaded in previous steps. The next step is to confirm they are loaded correctly by printing their types and a small part of the tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "collapsed": true,
        "id": "25b91ce7",
        "outputId": "5f5462c1-634b-4246-b9fc-16b8d94c0216"
      },
      "source": [
        "print(type(model))\n",
        "print(type(tokenizer))\n",
        "print(tokenizer.vocab_size)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2195595209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d64d86fb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to print the types of the model and tokenizer failed because they were not defined. This indicates the previous code cell where they were loaded might not have been executed correctly or there was an issue with the kernel. The next step is to re-load the model and tokenizer to ensure they are available for use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571,
          "referenced_widgets": [
            "2739456193df4faab7a1c7c7c84a2375",
            "89f805c3d76f460d951a9b5babe0d767",
            "9582b930011e4e129846468bec2c8ab4",
            "33fff551cd0949c1b2d1207ea519c30b",
            "29f543dc20594141b00b5216e941515a",
            "c6d1f35da4a743a28a9dc603a35e780f",
            "5d9df8e6394240949cc9849af93ce53c",
            "1335178132914f69a48185468a305803",
            "7089f7d501e447f1829d4ba497af4c8d",
            "d28268a0d9724d3cacb959eb05ecb60a",
            "c26c442a5730401e801f810281ad7cc6",
            "89ecef33c6224d0a8d8db7fca9cc0b0f",
            "034d20eac4fd45f2a6d283b7a745d0e4",
            "b618be90895a4370987c039ac2234fdc",
            "698359e794b840aaa9292a9a3d05cfda",
            "ef98761c28c048dabf8e5cc83a993b92",
            "3b5875f09a644597aa1e1a0753656cb5",
            "24386ac035814db98574667ebf1e5ea5",
            "806748f77ade42d29e460ffd10a7d2bd",
            "1fcb0ea830754a5cbd8cbe435951963e",
            "d315a0b18fea4942ad8b290aa3235b40",
            "10086424f1b2470eb41b86732619b4ba",
            "5ed77893bf694b20b313b11d01b7a79d",
            "0c007eed97c54aeca1a898ab07bd6f50",
            "4d6b8d4ed14b43f1a4d47d4a41eee86f",
            "30f7a0be25cb432ab55023c936600a4c",
            "b20dbf70b37e4ea2a895133afdf13da9",
            "9cca8200283d4958b2d43e8710e967ae",
            "2b55ee615af14053ba3a58ed9bbc5f2b",
            "41322396a18f49d697289ff63a190cc6",
            "577a9922421c408e87301a4948be8a33",
            "54716dcd2f2e4fe0aabd88a4ae49d2a5",
            "11974ebd72ee480ab8f1510e4d4ee433",
            "e242d6b20d664b46a8a3dbdcf5cc34db",
            "07f79c45bddd43f2bad5631fc0e30d37",
            "f84f574e89854a8682236d8138797916",
            "471d480ce6394c74bec51d751f484d0b",
            "e450defc6e934f88acd756654660a295",
            "d40d9c3c70d8415895356826915ccb4c",
            "ac24ed68a66c4974b9a73e786ab10d1e",
            "9b297540c57f44ae9efd43294c2b80d2",
            "e652e4350d67484782288d7959214ba0",
            "63a1c85dc7454d31aac028cf209e8a1d",
            "a5cda010ff84435284f80212955804a3",
            "880e11b958044e88ab47caa1d3fe1966",
            "437a021bca5d4ef0b8e1a3c487df4e91",
            "8b5994c1f45549d3ae414179cbb3a6e2",
            "e2001f411dc54cd1802efb37dd2b8400",
            "3d3c99ad470f4f5886818432e480826f",
            "5f80429226be4bdbb74377ded4656902",
            "3c016e00195749008707b9b2f391a883",
            "70e899dce4964ccea23dab2c9290fc87",
            "6afdce93ecd94be7bf8ee134364b7265",
            "2dacbac2601c4af38d1e033751edc037",
            "2a079cb26702492489cef98de1a04d86",
            "56d386e21dba41a4a07971e8d21ead6f",
            "67dbb1f211924d52b46a17a5b957f1e4",
            "1dc9e49cf8b0441593915925ca1595fe",
            "10c066643e074dfbbfe922e409abe41b",
            "705da69fac0f4b969e6a17d20feb64b0",
            "c3a0e11eba6c46bf91bf6353a4cc4ed8",
            "75016ac567614168963517082343208c",
            "4e8b71f26b0d42488643c73bea2b6271",
            "e7e7051ee6424aa8ae7a13712525ba77",
            "23e6aeaff1ec44538f31b1f84d993eb4",
            "59501bc60f8f49729daf75b58dc9462f",
            "9e436fc2d139463dafea6f9f684fafb5",
            "441bf22270ab41c687670629fd7c14a4",
            "41e2e7576d0d4cfa8e13f91daeae905d",
            "2c019f4c0a4845cd993a99ea517065cc",
            "3a4e54f2897746bf8aead99ba034ea82",
            "5fb802ffcba247c281a5b79145c8a949",
            "19e519a45fdd47b6b9c5df3da10d3f42",
            "94a1f0edc9b4493fa731060d45ad83df",
            "6bab14d96b224a4897e827466a0ea5ec",
            "277406fc10ae4327b2b24fcf934411ef",
            "1c946ad33d98441ebe071f868c44114f"
          ]
        },
        "id": "fuixZIxQhvEi",
        "outputId": "c9163cd7-6633-4356-b1d5-0428a94edcfa"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "\n",
        "print(type(model))\n",
        "print(type(tokenizer))\n",
        "print(tokenizer.vocab_size)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_xla/experimental/gru.py:113: SyntaxWarning: invalid escape sequence '\\_'\n",
            "  * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` or\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/793 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2739456193df4faab7a1c7c7c84a2375"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89ecef33c6224d0a8d8db7fca9cc0b0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ed77893bf694b20b313b11d01b7a79d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e242d6b20d664b46a8a3dbdcf5cc34db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "880e11b958044e88ab47caa1d3fe1966"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56d386e21dba41a4a07971e8d21ead6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e436fc2d139463dafea6f9f684fafb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
            "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
            "32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "995e5b6a"
      },
      "source": [
        "## Gather and preprocess the dataset\n",
        "\n",
        "### Subtask:\n",
        "Collect or create a dataset of natural language descriptions paired with corresponding complete Android project structures. Preprocess the data into a format suitable for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54b2f140"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the characteristics of the dataset needed for the task, outline potential strategies for obtaining it, and detail the necessary preprocessing steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f822b9f",
        "outputId": "752ff047-50e8-4b76-9314-6653ac20bfd3"
      },
      "source": [
        "# 1. Dataset Characteristics\n",
        "\n",
        "print(\"1. Dataset Characteristics:\")\n",
        "print(\"The dataset should consist of pairs of natural language descriptions and complete Android project structures. Each natural language description should clearly and comprehensively describe the desired functionality and UI of an Android application.\")\n",
        "print(\"The corresponding Android project structure should be a complete, functional project including all necessary files (e.g., AndroidManifest.xml, Java/Kotlin source files, layout XML files, resource files, build.gradle files) that can be built into an APK.\")\n",
        "print(\"Ideal Format:\")\n",
        "print(\"- Natural Language Description: A string containing the description.\")\n",
        "print(\"- Android Project Structure: This could be represented as a compressed archive (like a .zip file) or a structured representation like a JSON object where keys represent file paths and values represent file contents.\")\n",
        "\n",
        "# 2. Strategies for Obtaining Dataset\n",
        "\n",
        "print(\"\\n2. Strategies for Obtaining Dataset:\")\n",
        "print(\"- Utilizing Existing Open-Source Android Projects: Find open-source Android projects on platforms like GitHub. Manually or semi-automatically create natural language descriptions that accurately describe the functionality of these projects. This requires understanding the project's code and features.\")\n",
        "print(\"- Generating Synthetic Data: This is more challenging for complex Android projects but could involve defining templates or using rule-based systems to generate simple project structures and corresponding descriptions. Advanced techniques might involve using other AI models to generate code based on descriptions, though this introduces a dependency.\")\n",
        "print(\"- Crowdsourcing: If resources allow, a platform could be set up to crowdsource the creation of descriptions for existing projects or even the creation of simple projects based on descriptions.\")\n",
        "\n",
        "# 3. Preprocessing Steps\n",
        "\n",
        "print(\"\\n3. Preprocessing Steps:\")\n",
        "print(\"- For Natural Language Descriptions: Tokenization is crucial. This involves breaking down the text into individual words or sub-word units. Standard NLP tokenization techniques can be used. Cleaning steps like removing special characters, handling punctuation, and potentially lowercasing the text might be necessary depending on the chosen model.\")\n",
        "print(\"- For Android Project Structures: If the project is in a compressed format, it needs to be extracted. The structure needs to be represented in a way that the model can process. If using a JSON representation, this is already structured. If dealing with raw files, they might need to be concatenated or processed individually. Tokenization of code files (using a code-specific tokenizer if available, or the same tokenizer as for natural language if not) is required.\")\n",
        "print(\"- Pairing and Formatting: Ensure each description is correctly paired with its corresponding project structure. The combined data needs to be formatted into the input format expected by the fine-tuning process, which typically involves creating input-output pairs. For example, the input might be the natural language description, and the output might be the tokenized representation of the Android project structure.\")\n",
        "print(\"- Handling Large Files/Projects: Android projects can be large and contain many files. Strategies for handling this might include focusing on smaller, simpler projects initially, or developing methods to process project structures in chunks.\")\n",
        "print(\"- Splitting Data: The dataset should be split into training, validation, and test sets.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Dataset Characteristics:\n",
            "The dataset should consist of pairs of natural language descriptions and complete Android project structures. Each natural language description should clearly and comprehensively describe the desired functionality and UI of an Android application.\n",
            "The corresponding Android project structure should be a complete, functional project including all necessary files (e.g., AndroidManifest.xml, Java/Kotlin source files, layout XML files, resource files, build.gradle files) that can be built into an APK.\n",
            "Ideal Format:\n",
            "- Natural Language Description: A string containing the description.\n",
            "- Android Project Structure: This could be represented as a compressed archive (like a .zip file) or a structured representation like a JSON object where keys represent file paths and values represent file contents.\n",
            "\n",
            "2. Strategies for Obtaining Dataset:\n",
            "- Utilizing Existing Open-Source Android Projects: Find open-source Android projects on platforms like GitHub. Manually or semi-automatically create natural language descriptions that accurately describe the functionality of these projects. This requires understanding the project's code and features.\n",
            "- Generating Synthetic Data: This is more challenging for complex Android projects but could involve defining templates or using rule-based systems to generate simple project structures and corresponding descriptions. Advanced techniques might involve using other AI models to generate code based on descriptions, though this introduces a dependency.\n",
            "- Crowdsourcing: If resources allow, a platform could be set up to crowdsource the creation of descriptions for existing projects or even the creation of simple projects based on descriptions.\n",
            "\n",
            "3. Preprocessing Steps:\n",
            "- For Natural Language Descriptions: Tokenization is crucial. This involves breaking down the text into individual words or sub-word units. Standard NLP tokenization techniques can be used. Cleaning steps like removing special characters, handling punctuation, and potentially lowercasing the text might be necessary depending on the chosen model.\n",
            "- For Android Project Structures: If the project is in a compressed format, it needs to be extracted. The structure needs to be represented in a way that the model can process. If using a JSON representation, this is already structured. If dealing with raw files, they might need to be concatenated or processed individually. Tokenization of code files (using a code-specific tokenizer if available, or the same tokenizer as for natural language if not) is required.\n",
            "- Pairing and Formatting: Ensure each description is correctly paired with its corresponding project structure. The combined data needs to be formatted into the input format expected by the fine-tuning process, which typically involves creating input-output pairs. For example, the input might be the natural language description, and the output might be the tokenized representation of the Android project structure.\n",
            "- Handling Large Files/Projects: Android projects can be large and contain many files. Strategies for handling this might include focusing on smaller, simpler projects initially, or developing methods to process project structures in chunks.\n",
            "- Splitting Data: The dataset should be split into training, validation, and test sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39141982"
      },
      "source": [
        "## Define the fine-tuning parameters\n",
        "\n",
        "### Subtask:\n",
        "Determine the hyperparameters for the fine-tuning process, such as learning rate, batch size, and number of epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2969a907"
      },
      "source": [
        "**Reasoning**:\n",
        "Determine appropriate hyperparameters for fine-tuning the model based on common practices for large language models for code generation and the task complexity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1f9e364",
        "outputId": "a7045de4-3058-4f90-946c-71240c49b307"
      },
      "source": [
        "# 1. Learning Rate: A smaller learning rate is generally preferred for fine-tuning pre-trained models.\n",
        "learning_rate = 5e-5\n",
        "\n",
        "# 2. Batch Size: Depends on available memory and dataset size. Smaller batches can add regularization.\n",
        "# For a task generating large outputs like code, a smaller batch size might be necessary due to memory constraints.\n",
        "batch_size = 4\n",
        "\n",
        "# 3. Number of Epochs: Depends on dataset size and convergence. Avoid overfitting with too many epochs on a small dataset.\n",
        "# Start with a relatively small number and monitor validation loss.\n",
        "num_epochs = 3\n",
        "\n",
        "# 4. Optimizer: AdamW is a common and effective optimizer for transformer models.\n",
        "optimizer_type = \"AdamW\"\n",
        "\n",
        "# 5. Learning Rate Schedule: Linear decay with warm-up is a common and effective schedule.\n",
        "lr_scheduler_type = \"linear\"\n",
        "warmup_steps = 0 # Number of steps for the learning rate to warm up. Can be adjusted.\n",
        "\n",
        "# 6. Weight Decay: Helps prevent overfitting.\n",
        "weight_decay = 0.01\n",
        "\n",
        "# 7. Dropout: Can be applied to different layers (e.g., attention, hidden) to prevent overfitting.\n",
        "# The pre-trained model likely has dropout configured, so we might use the default or adjust slightly.\n",
        "dropout_rate = 0.1 # Example value, check model's default configuration\n",
        "\n",
        "# 8. Gradient Accumulation Steps: Useful for effectively increasing the batch size when memory is limited.\n",
        "# If batch_size is already large enough for the hardware, set to 1.\n",
        "gradient_accumulation_steps = 8 # Accumulate gradients over 8 batches before updating weights\n",
        "\n",
        "print(\"Hyperparameters for Fine-tuning:\")\n",
        "print(f\"Learning Rate: {learning_rate}\")\n",
        "print(f\"Batch Size: {batch_size}\")\n",
        "print(f\"Number of Epochs: {num_epochs}\")\n",
        "print(f\"Optimizer: {optimizer_type}\")\n",
        "print(f\"Learning Rate Scheduler: {lr_scheduler_type}\")\n",
        "print(f\"Warmup Steps: {warmup_steps}\")\n",
        "print(f\"Weight Decay: {weight_decay}\")\n",
        "print(f\"Dropout Rate: {dropout_rate}\")\n",
        "print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
        "\n",
        "# Make a note of these hyperparameters for the next step.\n",
        "hyperparameters = {\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"optimizer_type\": optimizer_type,\n",
        "    \"lr_scheduler_type\": lr_scheduler_type,\n",
        "    \"warmup_steps\": warmup_steps,\n",
        "    \"weight_decay\": weight_decay,\n",
        "    \"dropout_rate\": dropout_rate,\n",
        "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters for Fine-tuning:\n",
            "Learning Rate: 5e-05\n",
            "Batch Size: 4\n",
            "Number of Epochs: 3\n",
            "Optimizer: AdamW\n",
            "Learning Rate Scheduler: linear\n",
            "Warmup Steps: 0\n",
            "Weight Decay: 0.01\n",
            "Dropout Rate: 0.1\n",
            "Gradient Accumulation Steps: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "affe9d7c"
      },
      "source": [
        "## Implement the fine-tuning code\n",
        "\n",
        "### Subtask:\n",
        "Write the code to load the model and data, define the training loop, and implement the optimization process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d084fb51"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries, load the model and tokenizer, create a dummy dataset, define training arguments, create a Trainer, and start the training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a143d027",
        "outputId": "049e6b91-7f49-4865-f2e3-a80916168c4a"
      },
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Load the pre-trained model and tokenizer (already loaded in previous steps, just ensure they are accessible)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "\n",
        "# Create a dummy dataset\n",
        "# In a real scenario, this would be your preprocessed dataset of tokenized input/output pairs.\n",
        "# This dummy dataset simulates the expected structure.\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, num_samples=10, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_samples = num_samples\n",
        "        self.max_length = max_length\n",
        "        self.dummy_data = []\n",
        "        for i in range(num_samples):\n",
        "            # Simulate tokenized input and output sequences\n",
        "            input_text = f\"Description of app {i+1}: Create a simple Android app.\"\n",
        "            output_code = f\"// Dummy code for app {i+1}\\nSystem.out.println(\\\"Hello, World!\\\");\"\n",
        "            encoded_input = tokenizer(input_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            encoded_output = tokenizer(output_code, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            # In a causal language model setting for fine-tuning, the labels are typically the next token in the sequence.\n",
        "            # For simplicity in this dummy example, we'll use the encoded_output as labels.\n",
        "            # In a real scenario, you might concatenate input and output and use the concatenated sequence as input and labels.\n",
        "            self.dummy_data.append({\n",
        "                \"input_ids\": encoded_input[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": encoded_input[\"attention_mask\"].squeeze(),\n",
        "                \"labels\": encoded_output[\"input_ids\"].squeeze() # Using output as labels for simplicity\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dummy_data[idx]\n",
        "\n",
        "# Instantiate the dummy dataset\n",
        "dummy_dataset = DummyDataset(tokenizer)\n",
        "\n",
        "# Define the training arguments\n",
        "# Using the hyperparameters defined in the previous step\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Output directory for checkpoints and predictions\n",
        "    learning_rate=hyperparameters[\"learning_rate\"],\n",
        "    per_device_train_batch_size=hyperparameters[\"batch_size\"],\n",
        "    num_train_epochs=hyperparameters[\"num_epochs\"],\n",
        "    weight_decay=hyperparameters[\"weight_decay\"],\n",
        "    gradient_accumulation_steps=hyperparameters[\"gradient_accumulation_steps\"],\n",
        "    # Add other relevant arguments as needed, e.g., evaluation strategy, logging steps\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True, # Set to False if you have a validation set\n",
        ")\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=dummy_dataset,         # training dataset\n",
        "    # eval_dataset=dummy_dataset           # evaluation dataset (optional)\n",
        ")\n",
        "\n",
        "# Start the training loop\n",
        "print(\"Starting fine-tuning...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning finished successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during fine-tuning: {e}\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred during fine-tuning: `fused=True` requires all the params to be floating point Tensors of supported devices: ['mps', 'cuda', 'xpu', 'hpu', 'cpu', 'privateuseone'] but torch.float32 and xla\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a13232d"
      },
      "source": [
        "**Reasoning**:\n",
        "The training failed because of an incompatibility with the device and the 'fused' parameter in the optimizer. Modify the TrainingArguments to explicitly disable fused optimizer if it's not supported, and try training again. Also, add a check for CUDA availability to potentially move the model to the GPU if available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MbDYVfooimZ_",
        "outputId": "bbda0562-742e-40e3-f76d-d09bf9543718"
      },
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Check for CUDA availability and move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    print(f\"Model moved to {device}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"Using {device}\")\n",
        "\n",
        "\n",
        "# Load the pre-trained model and tokenizer (already loaded)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "\n",
        "# Create a dummy dataset (already defined)\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, num_samples=10, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_samples = num_samples\n",
        "        self.max_length = max_length\n",
        "        self.dummy_data = []\n",
        "        for i in range(num_samples):\n",
        "            input_text = f\"Description of app {i+1}: Create a simple Android app.\"\n",
        "            output_code = f\"// Dummy code for app {i+1}\\nSystem.out.println(\\\"Hello, World!\\\");\"\n",
        "            encoded_input = tokenizer(input_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            encoded_output = tokenizer(output_code, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            self.dummy_data.append({\n",
        "                \"input_ids\": encoded_input[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": encoded_input[\"attention_mask\"].squeeze(),\n",
        "                \"labels\": encoded_output[\"input_ids\"].squeeze()\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dummy_data[idx]\n",
        "\n",
        "# Instantiate the dummy dataset\n",
        "dummy_dataset = DummyDataset(tokenizer)\n",
        "\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=hyperparameters[\"learning_rate\"],\n",
        "    per_device_train_batch_size=hyperparameters[\"batch_size\"],\n",
        "    num_train_epochs=hyperparameters[\"num_epochs\"],\n",
        "    weight_decay=hyperparameters[\"weight_decay\"],\n",
        "    gradient_accumulation_steps=hyperparameters[\"gradient_accumulation_steps\"],\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    # Explicitly disable fused optimizer if not supported by the device\n",
        "    # This might be a reason for the previous error.\n",
        "    optim=\"adamw_hf\" # Use the Hugging Face default AdamW optimizer which is generally compatible\n",
        ")\n",
        "\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dummy_dataset,\n",
        ")\n",
        "\n",
        "# Start the training loop\n",
        "print(\"Starting fine-tuning...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning finished successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during fine-tuning: {e}\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "adamw_hf is not a valid OptimizerNames, please select one of ['adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_torch_npu_fused', 'adamw_apex_fused', 'adafactor', 'adamw_anyprecision', 'adamw_torch_4bit', 'adamw_torch_8bit', 'ademamix', 'sgd', 'adagrad', 'adamw_bnb_8bit', 'adamw_8bit', 'ademamix_8bit', 'lion_8bit', 'lion_32bit', 'paged_adamw_32bit', 'paged_adamw_8bit', 'paged_ademamix_32bit', 'paged_ademamix_8bit', 'paged_lion_32bit', 'paged_lion_8bit', 'rmsprop', 'rmsprop_bnb', 'rmsprop_bnb_8bit', 'rmsprop_bnb_32bit', 'galore_adamw', 'galore_adamw_8bit', 'galore_adafactor', 'galore_adamw_layerwise', 'galore_adamw_8bit_layerwise', 'galore_adafactor_layerwise', 'lomo', 'adalomo', 'grokadamw', 'schedule_free_radam', 'schedule_free_adamw', 'schedule_free_sgd', 'apollo_adamw', 'apollo_adamw_layerwise', 'stable_adamw']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1829038996.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Define the training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./results\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, opti...\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lr_scheduler_type reduce_lr_on_plateau requires torch>=0.2.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimizerNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madafactor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m             warnings.warn(\n",
            "\u001b[0;32m/usr/lib/python3.12/enum.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, value, names, module, qualname, type, start, boundary, *values)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_not_given\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m         \u001b[0;31m# otherwise, functional API: we're creating a new Enum type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_not_given\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/enum.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                     \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mve_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;31m# ensure all variables that could hold an exception are destroyed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/enum.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_missing_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m_missing_\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_missing_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    481\u001b[0m             \u001b[0;34mf\"{value} is not a valid {cls.__name__}, please select one of {list(cls._value2member_map_.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: adamw_hf is not a valid OptimizerNames, please select one of ['adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_torch_npu_fused', 'adamw_apex_fused', 'adafactor', 'adamw_anyprecision', 'adamw_torch_4bit', 'adamw_torch_8bit', 'ademamix', 'sgd', 'adagrad', 'adamw_bnb_8bit', 'adamw_8bit', 'ademamix_8bit', 'lion_8bit', 'lion_32bit', 'paged_adamw_32bit', 'paged_adamw_8bit', 'paged_ademamix_32bit', 'paged_ademamix_8bit', 'paged_lion_32bit', 'paged_lion_8bit', 'rmsprop', 'rmsprop_bnb', 'rmsprop_bnb_8bit', 'rmsprop_bnb_32bit', 'galore_adamw', 'galore_adamw_8bit', 'galore_adafactor', 'galore_adamw_layerwise', 'galore_adamw_8bit_layerwise', 'galore_adafactor_layerwise', 'lomo', 'adalomo', 'grokadamw', 'schedule_free_radam', 'schedule_free_adamw', 'schedule_free_sgd', 'apollo_adamw', 'apollo_adamw_layerwise', 'stable_adamw']"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58173e53"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because 'adamw_hf' is not a valid optimizer name in the TrainingArguments. The traceback provides a list of valid optimizer names. I need to select a valid optimizer from the list provided in the error message, such as 'adamw_torch', which is a standard AdamW implementation compatible with PyTorch. I will update the `optim` parameter in `TrainingArguments` to use 'adamw_torch'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "4pueJYouiriG",
        "outputId": "ed305e9e-d166-4bf3-887a-5e1da6984af2"
      },
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Check for CUDA availability and move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    print(f\"Model moved to {device}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"Using {device}\")\n",
        "\n",
        "\n",
        "# Load the pre-trained model and tokenizer (already loaded)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "\n",
        "# Create a dummy dataset (already defined)\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, num_samples=10, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_samples = num_samples\n",
        "        self.max_length = max_length\n",
        "        self.dummy_data = []\n",
        "        for i in range(num_samples):\n",
        "            input_text = f\"Description of app {i+1}: Create a simple Android app.\"\n",
        "            output_code = f\"// Dummy code for app {i+1}\\nSystem.out.println(\\\"Hello, World!\\\");\"\n",
        "            encoded_input = tokenizer(input_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            encoded_output = tokenizer(output_code, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            self.dummy_data.append({\n",
        "                \"input_ids\": encoded_input[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": encoded_input[\"attention_mask\"].squeeze(),\n",
        "                \"labels\": encoded_output[\"input_ids\"].squeeze()\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dummy_data[idx]\n",
        "\n",
        "# Instantiate the dummy dataset\n",
        "dummy_dataset = DummyDataset(tokenizer)\n",
        "\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=hyperparameters[\"learning_rate\"],\n",
        "    per_device_train_batch_size=hyperparameters[\"batch_size\"],\n",
        "    num_train_epochs=hyperparameters[\"num_epochs\"],\n",
        "    weight_decay=hyperparameters[\"weight_decay\"],\n",
        "    gradient_accumulation_steps=hyperparameters[\"gradient_accumulation_steps\"],\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    # Use a valid optimizer name from the traceback list\n",
        "    optim=\"adamw_torch\"\n",
        ")\n",
        "\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dummy_dataset,\n",
        ")\n",
        "\n",
        "# Start the training loop\n",
        "print(\"Starting fine-tuning...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning finished successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during fine-tuning: {e}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/3 : < :, Epoch 1/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred during fine-tuning: Bad StatusOr access: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 43.00M. That was not possible. There are 26.13M free.; (0x0x0_HBM0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56149d90"
      },
      "source": [
        "**Reasoning**:\n",
        "The training failed due to a resource exhaustion error (out of memory) on the device (CPU in this case). This is likely because the model and batch size are too large for the available memory. To address this, I need to reduce the memory footprint. I can try reducing the `per_device_train_batch_size` and potentially increase `gradient_accumulation_steps` to compensate. I will also add a warning about potential long training times on CPU due to the large model size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmZ80uTDiyXk",
        "outputId": "bfc19985-d543-4c76-e94a-07d710a06f67"
      },
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Check for CUDA availability and move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    print(f\"Model moved to {device}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"Using {device}. Note: Training a model of this size on CPU can be very slow and resource-intensive.\")\n",
        "\n",
        "\n",
        "# Load the pre-trained model and tokenizer (already loaded)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "\n",
        "# Create a dummy dataset (already defined)\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, num_samples=10, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_samples = num_samples\n",
        "        self.max_length = max_length\n",
        "        self.dummy_data = []\n",
        "        for i in range(num_samples):\n",
        "            input_text = f\"Description of app {i+1}: Create a simple Android app.\"\n",
        "            output_code = f\"// Dummy code for app {i+1}\\nSystem.out.println(\\\"Hello, World!\\\");\"\n",
        "            encoded_input = tokenizer(input_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            encoded_output = tokenizer(output_code, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            self.dummy_data.append({\n",
        "                \"input_ids\": encoded_input[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": encoded_input[\"attention_mask\"].squeeze(),\n",
        "                \"labels\": encoded_output[\"input_ids\"].squeeze()\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dummy_data[idx]\n",
        "\n",
        "# Instantiate the dummy dataset\n",
        "dummy_dataset = DummyDataset(tokenizer)\n",
        "\n",
        "\n",
        "# Define the training arguments\n",
        "# Reducing batch size and increasing gradient accumulation steps to reduce memory usage\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=hyperparameters[\"learning_rate\"],\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "    num_train_epochs=hyperparameters[\"num_epochs\"],\n",
        "    weight_decay=hyperparameters[\"weight_decay\"],\n",
        "    gradient_accumulation_steps=hyperparameters[\"gradient_accumulation_steps\"] * hyperparameters[\"batch_size\"], # Increase accumulation steps\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    optim=\"adamw_torch\"\n",
        ")\n",
        "\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dummy_dataset,\n",
        ")\n",
        "\n",
        "# Start the training loop\n",
        "print(\"Starting fine-tuning...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning finished successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during fine-tuning: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu. Note: Training a model of this size on CPU can be very slow and resource-intensive.\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f06bf64f"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Execute the fine-tuning process using the prepared data and defined parameters. Monitor the training progress and adjust parameters as needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4af42295"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the fine-tuning process using the prepared data and defined parameters by calling `trainer.train()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb765d19",
        "outputId": "1e455b13-bf40-491f-8df2-a7db07448e83"
      },
      "source": [
        "print(\"Continuing fine-tuning...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning finished successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during fine-tuning: {e}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuing fine-tuning...\n",
            "An error occurred during fine-tuning: name 'trainer' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90e1228c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the `trainer` object was not defined. This is likely due to the kernel state being reset or the previous code block not being executed correctly. I need to re-execute the code block that defines and initializes the `trainer` object before calling `trainer.train()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "9f7f1044",
        "outputId": "bbdd5e7b-688e-4af4-8fc7-932102259641"
      },
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Check for CUDA availability and move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    print(f\"Model moved to {device}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"Using {device}. Note: Training a model of this size on CPU can be very slow and resource-intensive.\")\n",
        "\n",
        "\n",
        "# Load the pre-trained model and tokenizer (already loaded)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "\n",
        "# Create a dummy dataset (already defined)\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, num_samples=10, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_samples = num_samples\n",
        "        self.max_length = max_length\n",
        "        self.dummy_data = []\n",
        "        for i in range(num_samples):\n",
        "            input_text = f\"Description of app {i+1}: Create a simple Android app.\"\n",
        "            output_code = f\"// Dummy code for app {i+1}\\nSystem.out.println(\\\"Hello, World!\\\");\"\n",
        "            encoded_input = tokenizer(input_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            encoded_output = tokenizer(output_code, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            self.dummy_data.append({\n",
        "                \"input_ids\": encoded_input[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": encoded_input[\"attention_mask\"].squeeze(),\n",
        "                \"labels\": encoded_output[\"input_ids\"].squeeze()\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dummy_data[idx]\n",
        "\n",
        "# Instantiate the dummy dataset\n",
        "dummy_dataset = DummyDataset(tokenizer)\n",
        "\n",
        "\n",
        "# Define the training arguments\n",
        "# Reducing batch size and increasing gradient accumulation steps to reduce memory usage\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=hyperparameters[\"learning_rate\"],\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "    num_train_epochs=hyperparameters[\"num_epochs\"],\n",
        "    weight_decay=hyperparameters[\"weight_decay\"],\n",
        "    gradient_accumulation_steps=hyperparameters[\"gradient_accumulation_steps\"] * hyperparameters[\"batch_size\"], # Increase accumulation steps\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    optim=\"adamw_torch\"\n",
        ")\n",
        "\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dummy_dataset,\n",
        ")\n",
        "\n",
        "# Start the training loop\n",
        "print(\"Starting fine-tuning...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning finished successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during fine-tuning: {e}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu. Note: Training a model of this size on CPU can be very slow and resource-intensive.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-534881877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Instantiate the dummy dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdummy_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1905c5c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the `tokenizer` object was not defined. This is likely due to the kernel state being reset or the previous code block that loaded the tokenizer not being executed correctly. I need to re-execute the code block that defines and initializes the `tokenizer` object, along with the `model` object and the rest of the training setup code, before calling `trainer.train()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "1OX9rL2-j82t",
        "outputId": "f5b469c0-e5ed-43d4-cd61-2d4a4b854dfb"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "\n",
        "# Check for CUDA availability and move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    print(f\"Model moved to {device}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"Using {device}. Note: Training a model of this size on CPU can be very slow and resource-intensive.\")\n",
        "\n",
        "\n",
        "# Create a dummy dataset (already defined)\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, num_samples=10, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_samples = num_samples\n",
        "        self.max_length = max_length\n",
        "        self.dummy_data = []\n",
        "        for i in range(num_samples):\n",
        "            input_text = f\"Description of app {i+1}: Create a simple Android app.\"\n",
        "            output_code = f\"// Dummy code for app {i+1}\\nSystem.out.println(\\\"Hello, World!\\\");\"\n",
        "            encoded_input = tokenizer(input_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            encoded_output = tokenizer(output_code, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            self.dummy_data.append({\n",
        "                \"input_ids\": encoded_input[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": encoded_input[\"attention_mask\"].squeeze(),\n",
        "                \"labels\": encoded_output[\"input_ids\"].squeeze()\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dummy_data[idx]\n",
        "\n",
        "# Instantiate the dummy dataset\n",
        "dummy_dataset = DummyDataset(tokenizer)\n",
        "\n",
        "\n",
        "# Define the training arguments\n",
        "# Reducing batch size and increasing gradient accumulation steps to reduce memory usage\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=hyperparameters[\"learning_rate\"],\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "    num_train_epochs=hyperparameters[\"num_epochs\"],\n",
        "    weight_decay=hyperparameters[\"weight_decay\"],\n",
        "    gradient_accumulation_steps=hyperparameters[\"gradient_accumulation_steps\"] * hyperparameters[\"batch_size\"], # Increase accumulation steps\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    optim=\"adamw_torch\"\n",
        ")\n",
        "\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dummy_dataset,\n",
        ")\n",
        "\n",
        "# Start the training loop\n",
        "print(\"Starting fine-tuning...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning finished successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during fine-tuning: {e}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu. Note: Training a model of this size on CPU can be very slow and resource-intensive.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hyperparameters' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-854108564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m training_args = TrainingArguments(\n\u001b[1;32m     49\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./results\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Reduced batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hyperparameters' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "300a930a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the `hyperparameters` dictionary was not defined. This is likely due to the kernel state being reset or the previous code block that defined the hyperparameters not being executed correctly. I need to re-execute the code block that defines the `hyperparameters` dictionary, along with the code that loads the model and tokenizer, defines the dummy dataset, and sets up the trainer, before calling `trainer.train()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "tpwlFNurkBps",
        "outputId": "1bb0530f-812b-4fbb-f9d5-466df0921ab0"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import Dataset # Assuming you will use the `datasets` library for your data\n",
        "\n",
        "# Define the hyperparameters\n",
        "# 1. Learning Rate: A smaller learning rate is generally preferred for fine-tuning pre-trained models.\n",
        "learning_rate = 5e-5\n",
        "\n",
        "# 2. Batch Size: Depends on available memory and dataset size. Smaller batches can add regularization.\n",
        "# For a task generating large outputs like code, a smaller batch size might be necessary due to memory constraints.\n",
        "# Reduced batch size due to resource constraints\n",
        "batch_size = 1\n",
        "\n",
        "# 3. Number of Epochs: Depends on dataset size and convergence. Avoid overfitting with too many epochs on a small dataset.\n",
        "# Start with a relatively small number and monitor validation loss.\n",
        "num_epochs = 3\n",
        "\n",
        "# 4. Optimizer: AdamW is a common and effective optimizer for transformer models.\n",
        "optimizer_type = \"AdamW\"\n",
        "\n",
        "# 5. Learning Rate Schedule: Linear decay with warm-up is a common and effective schedule.\n",
        "lr_scheduler_type = \"linear\"\n",
        "warmup_steps = 0 # Number of steps for the learning rate to warm up. Can be adjusted.\n",
        "\n",
        "# 6. Weight Decay: Helps prevent overfitting.\n",
        "weight_decay = 0.01\n",
        "\n",
        "# 7. Dropout: Can be applied to different layers (e.g., attention, hidden) to prevent overfitting.\n",
        "# The pre-trained model likely has dropout configured, so we might use the default or adjust slightly.\n",
        "dropout_rate = 0.1 # Example value, check model's default configuration\n",
        "\n",
        "# 8. Gradient Accumulation Steps: Useful for effectively increasing the batch size when memory is limited.\n",
        "# If batch_size is already large enough for the hardware, set to 1.\n",
        "# Increased accumulation steps to compensate for smaller batch size\n",
        "gradient_accumulation_steps = 32 # Example: 32 * 1 = effective batch size of 32\n",
        "\n",
        "hyperparameters = {\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"optimizer_type\": optimizer_type,\n",
        "    \"lr_scheduler_type\": lr_scheduler_type,\n",
        "    \"warmup_steps\": warmup_steps,\n",
        "    \"weight_decay\": weight_decay,\n",
        "    \"dropout_rate\": dropout_rate,\n",
        "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "}\n",
        "\n",
        "print(\"Hyperparameters for Fine-tuning (Adjusted for potential memory constraints):\")\n",
        "print(f\"Learning Rate: {learning_rate}\")\n",
        "print(f\"Per Device Train Batch Size: {batch_size}\")\n",
        "print(f\"Number of Epochs: {num_epochs}\")\n",
        "print(f\"Optimizer: {optimizer_type}\")\n",
        "print(f\"Learning Rate Scheduler: {lr_scheduler_type}\")\n",
        "print(f\"Warmup Steps: {warmup_steps}\")\n",
        "print(f\"Weight Decay: {weight_decay}\")\n",
        "print(f\"Dropout Rate: {dropout_rate}\")\n",
        "print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
        "\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "\n",
        "# Check for CUDA availability and move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    print(f\"Model moved to {device}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"Using {device}. Note: Training a model of this size on CPU can be very slow and resource-intensive.\")\n",
        "\n",
        "\n",
        "# --- Placeholder for Loading and Preprocessing Your Android-Specific Dataset ---\n",
        "# In a real scenario, you would load and preprocess your actual dataset here.\n",
        "# This might involve loading from files, tokenizing the text and code, and formatting it\n",
        "# into a structure suitable for the Trainer, likely using the `datasets` library.\n",
        "\n",
        "# Example: Loading a dummy dataset structure\n",
        "# Replace this with your actual dataset loading and preprocessing logic\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, num_samples=10, max_length=512): # Increased max_length for code\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_samples = num_samples\n",
        "        self.max_length = max_length\n",
        "        self.dummy_data = []\n",
        "        for i in range(num_samples):\n",
        "            # Simulate tokenized input (description) and output (code/project structure) sequences\n",
        "            input_text = f\"Description of app {i+1}: Create a simple Android app with a button that displays a message.\"\n",
        "            # This is a highly simplified representation; a real dataset would have much more complex code\n",
        "            output_code = f\"\"\"// Dummy code for app {i+1}\n",
        "            package com.example.myapp{i};\n",
        "\n",
        "            import androidx.appcompat.app.AppCompatActivity;\n",
        "            import android.os.Bundle;\n",
        "            import android.view.View;\n",
        "            import android.widget.Button;\n",
        "            import android.widget.Toast;\n",
        "\n",
        "            public class MainActivity extends AppCompatActivity {{\n",
        "\n",
        "                @Override\n",
        "                protected void onCreate(Bundle savedInstanceState) {{\n",
        "                    super.onCreate(savedInstanceState);\n",
        "                    setContentView(R.layout.activity_main);\n",
        "\n",
        "                    Button myButton = findViewById(R.id.my_button);\n",
        "                    myButton.setOnClickListener(new View.OnClickListener() {{\n",
        "                        @Override\n",
        "                        public void onClick(View v) {{\n",
        "                            Toast.makeText(MainActivity.this, \"Button clicked!\", Toast.LENGTH_SHORT).show();\n",
        "                        }}\n",
        "                    }});\n",
        "                }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "            # Tokenize and prepare for causal language modeling (predicting next token)\n",
        "            # In a real scenario, you might concatenate input_text and output_code with special tokens\n",
        "            # and use the concatenated sequence for training. The labels would be the tokens of the output_code.\n",
        "            combined_text = input_text + tokenizer.eos_token + output_code # Example concatenation\n",
        "\n",
        "            encoded_sequence = tokenizer(\n",
        "                combined_text,\n",
        "                max_length=self.max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # For causal language modeling, labels are typically the input IDs shifted by one\n",
        "            labels = encoded_sequence[\"input_ids\"].clone()\n",
        "            # In a real scenario, you might want to mask the loss for the input part of the sequence\n",
        "            # so the model only learns to generate the output code.\n",
        "\n",
        "            self.dummy_data.append({\n",
        "                \"input_ids\": encoded_sequence[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": encoded_sequence[\"attention_mask\"].squeeze(),\n",
        "                \"labels\": labels.squeeze()\n",
        "            })\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dummy_data[idx]\n",
        "\n",
        "# Instantiate the dummy dataset (replace with your actual dataset)\n",
        "# Your actual dataset loading and preprocessing would go here, resulting in a `train_dataset`\n",
        "# For now, we use the dummy dataset as a placeholder to allow the code structure to run.\n",
        "# If you have your dataset ready in a format like a Hugging Face `Dataset` object,\n",
        "# you would replace the DummyDataset instantiation and potentially the __init__ method above\n",
        "# with code to load and prepare your data.\n",
        "\n",
        "# Example of how you might load a dataset using the `datasets` library:\n",
        "# from datasets import load_dataset\n",
        "# raw_dataset = load_dataset(\"your_dataset_name\", \"your_dataset_config\")\n",
        "# train_dataset = raw_dataset[\"train\"].map(your_preprocessing_function, batched=True)\n",
        "# eval_dataset = raw_dataset[\"validation\"].map(your_preprocessing_function, batched=True)\n",
        "\n",
        "# Using the DummyDataset as a placeholder:\n",
        "train_dataset = DummyDataset(tokenizer)\n",
        "# eval_dataset = DummyDataset(tokenizer) # Uncomment if you have an evaluation set\n",
        "\n",
        "\n",
        "# Define the training arguments\n",
        "# Using the hyperparameters defined above\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Output directory for checkpoints and predictions\n",
        "    learning_rate=hyperparameters[\"learning_rate\"],\n",
        "    per_device_train_batch_size=hyperparameters[\"batch_size\"],\n",
        "    num_train_epochs=hyperparameters[\"num_epochs\"],\n",
        "    weight_decay=hyperparameters[\"weight_decay\"],\n",
        "    gradient_accumulation_steps=hyperparameters[\"gradient_accumulation_steps\"],\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=1, # Log more frequently for small dataset\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    # prediction_loss_only=True, # Keep as True if no eval_dataset\n",
        "    evaluation_strategy=\"steps\" if 'eval_dataset' in locals() else \"no\", # Evaluate if eval_dataset is defined\n",
        "    eval_steps=10 if 'eval_dataset' in locals() else None,\n",
        "    save_strategy=\"steps\",\n",
        "    optim=\"adamw_torch\", # Use a valid optimizer name\n",
        ")\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    # eval_dataset=eval_dataset           # evaluation dataset (optional)\n",
        ")\n",
        "\n",
        "# Start the training loop\n",
        "print(\"Starting fine-tuning...\")\n",
        "print(\"Note: This will likely fail due to resource limitations with the current environment and dummy data.\")\n",
        "print(\"A real Android-specific dataset and a more powerful GPU are required for successful training.\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning finished successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during fine-tuning: {e}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4145017243.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m \u001b[0;31m# Assuming you will use the `datasets` library for your data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "861a4fdc",
        "outputId": "c9439eba-f8e6-4771-a4e1-9277136707eb"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-4.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.19 (from datasets)\n",
            "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.9.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Downloading datasets-4.4.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.5/511.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, propcache, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.4.0 dill-0.4.0 frozenlist-1.8.0 multiprocess-0.70.18 propcache-0.4.1 xxhash-3.6.0 yarl-1.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3307d7f"
      },
      "source": [
        "**Reasoning**:\n",
        "The fine-tuning process failed due to a `RESOURCE_EXHAUSTED` error, indicating insufficient memory. As per the instructions, I need to adjust the hyperparameters to reduce memory usage. I will further reduce `per_device_train_batch_size` and increase `gradient_accumulation_steps`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoppU6G_kmTC",
        "outputId": "9feab152-78ec-4cd6-f8c2-314a692ae3fc"
      },
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Check for CUDA availability and move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    print(f\"Model moved to {device}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"Using {device}. Note: Training a model of this size on CPU can be very slow and resource-intensive, and may fail due to memory constraints.\")\n",
        "\n",
        "\n",
        "# Load the pre-trained model and tokenizer (already loaded)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
        "\n",
        "# Create a dummy dataset (already defined)\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, num_samples=10, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_samples = num_samples\n",
        "        self.max_length = max_length\n",
        "        self.dummy_data = []\n",
        "        for i in range(num_samples):\n",
        "            input_text = f\"Description of app {i+1}: Create a simple Android app.\"\n",
        "            output_code = f\"// Dummy code for app {i+1}\\nSystem.out.println(\\\"Hello, World!\\\");\"\n",
        "            encoded_input = tokenizer(input_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            encoded_output = tokenizer(output_code, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            self.dummy_data.append({\n",
        "                \"input_ids\": encoded_input[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": encoded_input[\"attention_mask\"].squeeze(),\n",
        "                \"labels\": encoded_output[\"input_ids\"].squeeze()\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dummy_data[idx]\n",
        "\n",
        "# Instantiate the dummy dataset\n",
        "dummy_dataset = DummyDataset(tokenizer)\n",
        "\n",
        "\n",
        "# Define the training arguments\n",
        "# Reducing batch size and increasing gradient accumulation steps to reduce memory usage\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=hyperparameters[\"learning_rate\"],\n",
        "    per_device_train_batch_size=1,  # Keep batch size at 1\n",
        "    num_train_epochs=hyperparameters[\"num_epochs\"],\n",
        "    weight_decay=hyperparameters[\"weight_decay\"],\n",
        "    gradient_accumulation_steps=(hyperparameters[\"gradient_accumulation_steps\"] * hyperparameters[\"batch_size\"]) * 2, # Double the accumulation steps\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    optim=\"adamw_torch\"\n",
        ")\n",
        "\n",
        "# Create a new Trainer instance with adjusted arguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dummy_dataset,\n",
        ")\n",
        "\n",
        "# Start the training loop again\n",
        "print(\"Starting fine-tuning with adjusted parameters...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning finished successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during fine-tuning: {e}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning with adjusted parameters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred during fine-tuning: Bad StatusOr access: RESOURCE_EXHAUSTED: Error loading program: Attempting to reserve 3.28G at the bottom of memory. That was not possible. There are 968.79M free, 0B reserved, and 968.79M reservable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46d259e"
      },
      "source": [
        "## Evaluate the fine-tuned model\n",
        "\n",
        "### Subtask:\n",
        "Assess the performance of the fine-tuned model on a separate evaluation dataset. Evaluate metrics such as code correctness, buildability, and functionality of the generated applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d39935d0"
      },
      "source": [
        "**Reasoning**:\n",
        "The training failed due to resource limitations, so a practical evaluation cannot be performed. I will explain the evaluation metrics and process that *would* be used if training were successful, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89cb3eb2",
        "outputId": "a4679a21-66b2-4be5-fb06-7ea664d01830"
      },
      "source": [
        "print(\"Due to resource limitations leading to the failure of the fine-tuning process, a practical evaluation of the fine-tuned model's performance cannot be conducted at this time.\")\n",
        "print(\"\\nHowever, if the training had been successful, the evaluation process would involve assessing the model's ability to generate functional Android applications from natural language descriptions.\")\n",
        "print(\"\\nHere are the key metrics and the evaluation process that would be employed:\")\n",
        "\n",
        "print(\"\\n1. Code Correctness:\")\n",
        "print(\"   - This metric assesses whether the generated code is syntactically correct and follows best practices.\")\n",
        "print(\"   - Automated tools such as linters (e.g., Ktlint for Kotlin, Checkstyle for Java), static code analyzers (e.g., SonarQube), and potentially unit tests generated or applied to the code snippets would be used.\")\n",
        "print(\"   - The code should compile without errors or warnings related to syntax or basic code structure.\")\n",
        "\n",
        "print(\"\\n2. Buildability:\")\n",
        "print(\"   - This metric verifies if the complete generated Android project structure can be successfully built into a functional APK file.\")\n",
        "print(\"   - This would involve using the Android SDK build tools (e.g., Gradle) to compile, assemble, and package the application.\")\n",
        "print(\"   - A successful build indicates that all dependencies are correctly specified, resources are properly linked, and the project structure is valid.\")\n",
        "\n",
        "print(\"\\n3. Functionality:\")\n",
        "print(\"   - This is the most critical metric, evaluating whether the resulting APK performs the actions and exhibits the UI described in the original natural language prompt.\")\n",
        "print(\"   - Assessing functionality is challenging to automate completely.\")\n",
        "print(\"   - **Evaluation Dataset:** A separate dataset of natural language descriptions and their expected application behaviors (but *not* the code) would be used. The model would generate code for each description in this set.\")\n",
        "print(\"   - **Challenges in Automatic Evaluation:** Automatically verifying the full functionality of a GUI application is complex. It requires understanding the visual output, user interactions, and internal state changes.\")\n",
        "print(\"   - **Potential Approaches for Functionality Evaluation:**\")\n",
        "print(\"     - **Manual Testing:** Human testers would install the generated APKs on emulators or physical devices and manually verify if they meet the requirements specified in the natural language prompt. This is thorough but not scalable.\")\n",
        "print(\"     - **Automated UI Testing Frameworks:** Developing or utilizing sophisticated automated UI testing frameworks (e.g., Espresso, Appium) could automate some functionality checks. This would involve writing test scripts that interact with the generated app's UI and assert expected behaviors. However, creating these test scripts automatically based on natural language descriptions is a significant challenge in itself.\")\n",
        "print(\"     - **Hybrid Approaches:** A combination of automated checks for basic functionality and manual testing for complex interactions and UI nuances would likely be the most practical approach.\")\n",
        "\n",
        "print(\"\\nIn conclusion, while the methodology for evaluating code correctness, buildability, and functionality using a separate evaluation dataset is clear, the failure to fine-tune the model prevents a practical assessment of its performance at this time.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Due to resource limitations leading to the failure of the fine-tuning process, a practical evaluation of the fine-tuned model's performance cannot be conducted at this time.\n",
            "\n",
            "However, if the training had been successful, the evaluation process would involve assessing the model's ability to generate functional Android applications from natural language descriptions.\n",
            "\n",
            "Here are the key metrics and the evaluation process that would be employed:\n",
            "\n",
            "1. Code Correctness:\n",
            "   - This metric assesses whether the generated code is syntactically correct and follows best practices.\n",
            "   - Automated tools such as linters (e.g., Ktlint for Kotlin, Checkstyle for Java), static code analyzers (e.g., SonarQube), and potentially unit tests generated or applied to the code snippets would be used.\n",
            "   - The code should compile without errors or warnings related to syntax or basic code structure.\n",
            "\n",
            "2. Buildability:\n",
            "   - This metric verifies if the complete generated Android project structure can be successfully built into a functional APK file.\n",
            "   - This would involve using the Android SDK build tools (e.g., Gradle) to compile, assemble, and package the application.\n",
            "   - A successful build indicates that all dependencies are correctly specified, resources are properly linked, and the project structure is valid.\n",
            "\n",
            "3. Functionality:\n",
            "   - This is the most critical metric, evaluating whether the resulting APK performs the actions and exhibits the UI described in the original natural language prompt.\n",
            "   - Assessing functionality is challenging to automate completely.\n",
            "   - **Evaluation Dataset:** A separate dataset of natural language descriptions and their expected application behaviors (but *not* the code) would be used. The model would generate code for each description in this set.\n",
            "   - **Challenges in Automatic Evaluation:** Automatically verifying the full functionality of a GUI application is complex. It requires understanding the visual output, user interactions, and internal state changes.\n",
            "   - **Potential Approaches for Functionality Evaluation:**\n",
            "     - **Manual Testing:** Human testers would install the generated APKs on emulators or physical devices and manually verify if they meet the requirements specified in the natural language prompt. This is thorough but not scalable.\n",
            "     - **Automated UI Testing Frameworks:** Developing or utilizing sophisticated automated UI testing frameworks (e.g., Espresso, Appium) could automate some functionality checks. This would involve writing test scripts that interact with the generated app's UI and assert expected behaviors. However, creating these test scripts automatically based on natural language descriptions is a significant challenge in itself.\n",
            "     - **Hybrid Approaches:** A combination of automated checks for basic functionality and manual testing for complex interactions and UI nuances would likely be the most practical approach.\n",
            "\n",
            "In conclusion, while the methodology for evaluating code correctness, buildability, and functionality using a separate evaluation dataset is clear, the failure to fine-tune the model prevents a practical assessment of its performance at this time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19a85b45"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The chosen base model for this task is \"deepseek-ai/deepseek-coder-1.3b-base,\" a large language model pre-trained on code.\n",
        "*   The dataset required for fine-tuning should consist of pairs of natural language descriptions and complete, functional Android project structures, including all necessary files.\n",
        "*   Potential strategies for obtaining the dataset include leveraging existing open-source projects by creating descriptions for them, generating synthetic data (though complex for this task), and crowdsourcing.\n",
        "*   Preprocessing steps for the dataset involve tokenization and cleaning for natural language descriptions, and for Android project structures, extraction, representation in a model-understandable format (like tokenization of code files), pairing, formatting into input/output pairs, handling large projects, and splitting into training, validation, and test sets.\n",
        "*   Hyperparameters for fine-tuning were determined, including a learning rate of 5e-5, a batch size of 4, 3 epochs, AdamW optimizer, linear learning rate schedule, 0.01 weight decay, 0.1 dropout rate, and 8 gradient accumulation steps.\n",
        "*   Attempts to implement and execute the fine-tuning process using the `transformers` library and a `Trainer` instance with a dummy dataset failed due to insufficient memory (specifically, GPU memory). The `RESOURCE_EXHAUSTED` error persisted despite adjusting batch size and gradient accumulation steps.\n",
        "*   Due to the failure of the fine-tuning process caused by resource limitations, a practical evaluation of the model could not be conducted.\n",
        "*   The theoretical evaluation process would involve assessing code correctness (syntax, best practices, compilation), buildability (successful APK creation), and functionality (matching the natural language description) using a separate evaluation dataset. Evaluating functionality of GUI apps is challenging and would likely require manual testing, automated UI testing frameworks, or a hybrid approach.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Acquiring or creating a large, high-quality dataset of natural language descriptions paired with corresponding functional Android project structures is a significant hurdle and a critical next step before fine-tuning can effectively begin.\n",
        "*   Fine-tuning a model of the size of \"deepseek-coder-1.3b-base\" for this complex task requires substantial computational resources, particularly GPU memory. Accessing a more powerful environment or exploring techniques like LoRA (Low-Rank Adaptation) or QLoRA for more memory-efficient fine-tuning would be necessary to overcome the `RESOURCE_EXHAUSTED` errors encountered.\n"
      ]
    }
  ]
}